Neural networks have grown larger in scale, making it difficult to understand their decision-making processes. This inability to examine the reasoning behind a networkâ€™s output makes it difficult to trust its results. Uncertainty quantification aims to improve human trust in a neural network by describing the scale of its uncertainty. Conformal prediction is a method of uncertainty quantification that generates confidence intervals and can be applied to any machine learning model, and has shown promising results in various experiments. It splits model data into training, calibration, and testing sets, then uses the calibration model to generate a threshold value which can be used to generate confidence intervals.

However, conformal prediction has several limitations that must be explored and overcome; in this report, we examine three such limitations. First, conformal prediction relies on the assumption that the calibration set is from the same distribution as the test set; in many situations, this will not be the case, causing the confidence intervals to be inaccurate. Second, conformal prediction requires a certain size of the calibration set; below this size, the confidence intervals are also no longer accurate. Thirdly, conformal prediction is sensitive to internal shifts. If the training and testing data have the same type of data, but it is not distributed the same conformal prediction will also lose accuracy. By exploring these limitations in depth, our findings seek to lay the groundwork for finding solutions to these problems in order to make conformal prediction a more robust method of uncertainty quantification.



word count: 250
